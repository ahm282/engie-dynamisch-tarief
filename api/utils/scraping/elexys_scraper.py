"""
Web scraper for Elexys electricity price data.

This module scrapes electricity price data from the Elexys Spot Belpex page,
processes the data, and stores it in the database while avoiding duplicates.
"""

from api.config.database import DatabaseManager
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from typing import List, Dict, Optional, Tuple
import sys
import os

# Add the project root to the path for imports
sys.path.append(os.path.dirname(os.path.dirname(
    os.path.dirname(os.path.abspath(__file__)))))


class ElexysElectricityScraper:
    """
    Scraper for Elexys electricity price data.

    Features:
    - Scrapes data from Elexys Spot Belpex table
    - Processes date/time and price information
    - Checks for existing data to avoid duplicates
    - Calculates consumer prices with markup
    - Robust error handling and logging
    """

    def __init__(self, db_path: str = None):
        """
        Initialize the scraper.

        Args:
            db_path: Optional path to the database file
        """
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # Database setup
        if db_path:
            self.db_path = db_path
        else:
            # Calculate absolute path more reliably
            # This file is in: api/utils/scraping/elexys_scraper.py
            # Database is at: api/db/electricity_prices.db
            current_file = os.path.abspath(__file__)
            utils_dir = os.path.dirname(current_file)  # api/utils/scraping
            api_dir = os.path.dirname(os.path.dirname(utils_dir))  # api
            self.db_path = os.path.join(api_dir, 'db', 'electricity_prices.db')

        # Ensure database directory exists
        db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
            self.logger.info(f"Created database directory: {db_dir}")

        self.logger.info(f"Using database path: {self.db_path}")

        self.db_manager = DatabaseManager()

        # Scraping configuration
        self.base_url = "https://my.elexys.be/MarketInformation/SpotBelpex.aspx"
        self.table_id = "contentPlaceHolder_belpexFilterGrid_DXMainTable"
        self.row_class_prefix = "contentPlaceHolder_belpexFilterGrid_DXDataRow"

        # Session for persistent connections
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
        })

    def fetch_webpage(self) -> Optional[BeautifulSoup]:
        """
        Fetch the Elexys webpage and parse it.

        Returns:
            BeautifulSoup object or None if failed
        """
        try:
            self.logger.info(f"Fetching webpage: {self.base_url}")
            response = self.session.get(self.base_url, timeout=30)
            response.raise_for_status()

            # Parse the HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            self.logger.info("Successfully fetched and parsed webpage")
            return soup

        except requests.RequestException as e:
            self.logger.error(f"Error fetching webpage: {e}")
            return None
        except Exception as e:
            self.logger.error(f"Unexpected error parsing webpage: {e}")
            return None

    def extract_table_data(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """
        Extract price data from the table.

        Args:
            soup: BeautifulSoup object of the webpage

        Returns:
            List of dictionaries containing the extracted data
        """
        try:
            # Find the main table
            table = soup.find('table', {'id': self.table_id})
            if not table:
                self.logger.error(f"Table with ID '{self.table_id}' not found")
                return []

            # Find all data rows using the correct ID pattern
            data_rows = []
            row_index = 0

            while True:
                row_id = f"{self.row_class_prefix}{row_index}"
                row = soup.find('tr', {'id': row_id})

                if not row:
                    # Try with class instead of ID
                    row = soup.find(
                        'tr', class_=f"{self.row_class_prefix}{row_index}")
                    if not row:
                        break

                # Extract date and price from the row
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    date_cell = cells[0].get_text(strip=True)
                    price_cell = cells[1].get_text(strip=True)

                    # Only add rows that have valid date and price data
                    if date_cell and price_cell and '/' in date_cell and 'â‚¬' in price_cell:
                        data_rows.append({
                            'date_time': date_cell,
                            'price_raw': price_cell,
                            'row_index': row_index
                        })

                row_index += 1

                # Safety limit to prevent infinite loops
                if row_index > 300:
                    break

            self.logger.info(f"Extracted {len(data_rows)} rows from table")
            return data_rows

        except Exception as e:
            self.logger.error(f"Error extracting table data: {e}")
            return []

    def parse_datetime(self, date_time_str: str) -> Optional[Tuple[datetime, str, int]]:
        """
        Parse date and time from the scraped string.

        Args:
            date_time_str: Date/time string like "18/07/2025 23:00:00"

        Returns:
            Tuple of (datetime object, date string, hour) or None if parsing fails
        """
        try:
            # Remove any extra whitespace and normalize
            date_time_str = date_time_str.strip()

            # Parse the datetime string
            # Expected format: "dd/mm/yyyy hh:mm:ss"
            dt = datetime.strptime(date_time_str, "%d/%m/%Y %H:%M:%S")

            # Format date as YYYY-MM-DD for database consistency
            date_str = dt.strftime("%Y-%m-%d")
            hour = dt.hour

            return dt, date_str, hour

        except ValueError as e:
            self.logger.error(f"Error parsing datetime '{date_time_str}': {e}")
            return None

    def parse_price(self, price_str: str) -> Optional[float]:
        """
        Parse price from the scraped string.

        Args:
            price_str: Price string like "â‚¬ 113,87"

        Returns:
            Price as float in EUR/MWh or None if parsing fails
        """
        try:
            # Remove currency symbol, spaces, and normalize
            price_str = price_str.replace('â‚¬', '').replace(' ', '').strip()

            # Handle European decimal format (comma as decimal separator)
            price_str = price_str.replace(',', '.')

            # Convert to float
            price = float(price_str)

            return price

        except (ValueError, TypeError) as e:
            self.logger.error(f"Error parsing price '{price_str}': {e}")
            return None

    def calculate_consumer_price(self, wholesale_price: float) -> float:
        """
        Calculate consumer price from wholesale price.

        Args:
            wholesale_price: Wholesale price in EUR/MWh

        Returns:
            Consumer price in euro cents per kWh
        """
        # Use the corrected consumer price formula: 1.3163 + (0.1019 * wholesale)
        consumer_price = 1.3163 + (0.1019 * wholesale_price)

        return round(consumer_price, 4)

    def check_existing_record(self, timestamp: datetime) -> Optional[Dict[str, any]]:
        """
        Check if a record already exists in the database and return its data.

        Args:
            timestamp: The timestamp to check

        Returns:
            Dictionary with existing record data or None if record doesn't exist
        """
        try:
            conn = self.db_manager.get_connection()
            cursor = conn.execute(
                "SELECT price_eur, price_raw, consumer_price_cents_kwh FROM electricity_prices WHERE timestamp = ?",
                (timestamp.strftime("%Y-%m-%d %H:%M:%S"),)
            )
            result = cursor.fetchone()
            conn.close()

            if result:
                return {
                    'price_eur': result[0],
                    'price_raw': result[1],
                    'consumer_price_cents_kwh': result[2]
                }
            return None

        except Exception as e:
            self.logger.error(f"Error checking existing record: {e}")
            return None

    def update_price_record(self, timestamp: datetime, date_str: str, hour: int,
                            price_eur: float, price_raw: str, consumer_price: float) -> bool:
        """
        Update an existing price record in the database.

        Args:
            timestamp: Datetime object
            date_str: Date string in YYYY-MM-DD format
            hour: Hour of the day (0-23)
            price_eur: Wholesale price in EUR/MWh
            price_raw: Raw price string from website
            consumer_price: Consumer price in cents/kWh

        Returns:
            True if successful, False otherwise
        """
        try:
            conn = self.db_manager.get_connection()

            cursor = conn.execute("""
                UPDATE electricity_prices 
                SET date = ?, hour = ?, price_eur = ?, price_raw = ?, consumer_price_cents_kwh = ?
                WHERE timestamp = ?
            """, (
                date_str,
                hour,
                price_eur,
                price_raw,
                consumer_price,
                timestamp.strftime("%Y-%m-%d %H:%M:%S")
            ))

            conn.commit()
            conn.close()

            self.logger.info(
                f"Updated record: {timestamp} - {price_eur} EUR/MWh")
            return True

        except Exception as e:
            self.logger.error(f"Error updating record: {e}")
            return False

    def insert_price_record(self, timestamp: datetime, date_str: str, hour: int,
                            price_eur: float, price_raw: str, consumer_price: float) -> bool:
        """
        Insert a price record into the database.

        Args:
            timestamp: Datetime object
            date_str: Date string in YYYY-MM-DD format
            hour: Hour of the day (0-23)
            price_eur: Wholesale price in EUR/MWh
            price_raw: Raw price string from website
            consumer_price: Consumer price in cents/kWh

        Returns:
            True if successful, False otherwise
        """
        try:
            conn = self.db_manager.get_connection()

            cursor = conn.execute("""
                INSERT INTO electricity_prices 
                (timestamp, date, hour, price_eur, price_raw, consumer_price_cents_kwh)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                date_str,
                hour,
                price_eur,
                price_raw,
                consumer_price
            ))

            conn.commit()
            conn.close()

            self.logger.debug(
                f"Inserted record: {timestamp} - {price_eur} EUR/MWh")
            return True

        except Exception as e:
            self.logger.error(f"Error inserting record: {e}")
            return False

    def process_scraped_data(self, data_rows: List[Dict[str, str]]) -> Dict[str, int]:
        """
        Process scraped data and insert/update in database.

        Args:
            data_rows: List of scraped data dictionaries

        Returns:
            Dictionary with processing statistics
        """
        stats = {
            'total_rows': len(data_rows),
            'processed': 0,
            'skipped_existing': 0,
            'skipped_errors': 0,
            'inserted': 0,
            'updated': 0
        }

        # Process rows in reverse order (oldest first)
        for row_data in reversed(data_rows):
            stats['processed'] += 1

            # Parse datetime
            datetime_result = self.parse_datetime(row_data['date_time'])
            if not datetime_result:
                stats['skipped_errors'] += 1
                continue

            timestamp, date_str, hour = datetime_result

            # Parse price
            price_eur = self.parse_price(row_data['price_raw'])
            if price_eur is None:
                stats['skipped_errors'] += 1
                continue

            # Calculate consumer price
            consumer_price = self.calculate_consumer_price(price_eur)

            # Check if record already exists
            existing_record = self.check_existing_record(timestamp)

            if existing_record:
                # Compare prices to see if update is needed
                price_diff = abs(existing_record['price_eur'] - price_eur)
                consumer_price_diff = abs(
                    existing_record['consumer_price_cents_kwh'] - consumer_price)

                # Update if there's a significant difference (more than 0.01 EUR/MWh or 0.001 cents/kWh)
                if price_diff > 0.01 or consumer_price_diff > 0.001 or existing_record['price_raw'] != row_data['price_raw']:
                    self.logger.info(
                        f"Price difference detected for {timestamp}: "
                        f"DB={existing_record['price_eur']}, Website={price_eur}, "
                        f"updating record"
                    )

                    if self.update_price_record(
                        timestamp, date_str, hour, price_eur,
                        row_data['price_raw'], consumer_price
                    ):
                        stats['updated'] += 1
                    else:
                        stats['skipped_errors'] += 1
                else:
                    self.logger.debug(
                        f"Record already exists for {timestamp} with same price, skipping")
                    stats['skipped_existing'] += 1
            else:
                # Insert new record
                if self.insert_price_record(
                    timestamp, date_str, hour, price_eur,
                    row_data['price_raw'], consumer_price
                ):
                    stats['inserted'] += 1
                else:
                    stats['skipped_errors'] += 1

        return stats

    def scrape_and_update(self) -> Dict[str, int]:
        """
        Main method to scrape data and update the database.

        Returns:
            Dictionary with scraping statistics
        """
        self.logger.info("Starting Elexys electricity price scraping")

        try:
            # Fetch webpage
            soup = self.fetch_webpage()
            if not soup:
                return {'error': 'Failed to fetch webpage'}

            # Extract table data
            data_rows = self.extract_table_data(soup)
            if not data_rows:
                return {'error': 'No data extracted from table'}

            # Process and insert data
            stats = self.process_scraped_data(data_rows)

            self.logger.info(f"Scraping completed: {stats}")
            return stats

        except Exception as e:
            self.logger.error(f"Error during scraping: {e}")
            return {'error': str(e)}

    def get_latest_db_timestamp(self) -> Optional[datetime]:
        """
        Get the latest timestamp from the database.

        Returns:
            Latest datetime or None if no data
        """
        try:
            conn = self.db_manager.get_connection()
            cursor = conn.execute(
                "SELECT MAX(timestamp) FROM electricity_prices"
            )
            result = cursor.fetchone()[0]
            conn.close()

            if result:
                return datetime.strptime(result, "%Y-%m-%d %H:%M:%S")
            return None

        except Exception as e:
            self.logger.error(f"Error getting latest timestamp: {e}")
            return None

    def print_stats(self, stats: Dict[str, int]):
        """Print scraping statistics in a readable format."""
        print("\n" + "="*50)
        print("ELEXYS SCRAPING RESULTS")
        print("="*50)

        if 'error' in stats:
            print(f"âŒ Error: {stats['error']}")
            return

        print(f"ğŸ“Š Total rows found: {stats.get('total_rows', 0)}")
        print(f"ğŸ”„ Rows processed: {stats.get('processed', 0)}")
        print(f"âœ… New records inserted: {stats.get('inserted', 0)}")
        print(f"ğŸ”„ Records updated: {stats.get('updated', 0)}")
        print(
            f"â­ï¸  Existing records skipped: {stats.get('skipped_existing', 0)}")
        print(f"âŒ Rows with errors: {stats.get('skipped_errors', 0)}")

        if stats.get('inserted', 0) > 0:
            print(
                f"\nğŸ‰ Successfully added {stats['inserted']} new price records!")

        if stats.get('updated', 0) > 0:
            print(
                f"\nğŸ”„ Successfully updated {stats['updated']} existing price records!")

        if stats.get('inserted', 0) == 0 and stats.get('updated', 0) == 0 and stats.get('skipped_existing', 0) > 0:
            print(f"\nâœ¨ Database is up to date - no new records needed")
        elif stats.get('inserted', 0) == 0 and stats.get('updated', 0) == 0:
            print(f"\nâš ï¸  No new records were added or updated")

        print("="*50)


def main():
    """Main function for command-line usage."""
    scraper = ElexysElectricityScraper()

    # Show current database status
    latest_timestamp = scraper.get_latest_db_timestamp()
    if latest_timestamp:
        print(f"ğŸ“… Latest data in database: {latest_timestamp}")
    else:
        print("ğŸ“… No existing data in database")

    # Run scraping
    stats = scraper.scrape_and_update()
    scraper.print_stats(stats)


if __name__ == "__main__":
    main()
